{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*这是一个股票数据分析方法的说明, 基本思路如下:*\n",
    "1. 历史数据中, 总有很多相似形状的走势, 这些形状有一些是可能预示后边走势的概率的\n",
    "2. 形状的处理上, 通过滚动切片的方式分割日k线数据, 并以每段的第一个k线收盘价作为基数, 整个线段只保留收盘价相对基数的涨幅数据,\n",
    "      这实际是一个归一化的处理过程, 股票和时段 价格的差异都抹去了, 只保留走势的形状片段\n",
    "3. 每个片段实际上就是一个向量, 且所有的片段长度相同, 起始点都是0, 所以以kmeans聚类方法对数据进行分类的话, 实际上就是将相近形状的片段聚类\n",
    "4. 每个片段都对应后续一段时间的走势情况, 保留n个交易日的最高价和最低价与片段最后一个收盘价的涨幅\n",
    "5. 使用聚类计算的结果, 对后续走势的情况进行分类, 然后统计每一类的最高 最低涨幅的相对某个值的分布, 能发现某些类别中的分布有明显不同\n",
    "6. 对分布比例相对突出的类别, 画出其均线形状, 可以发现有比较近似的形状\n",
    "7. 根据这类形状, 继续以之前聚类的模型对最新的数据进行分类, 寻找相似的形状, 这是一种选股方法\n",
    "\n",
    "  是否有效, 还在验证过程中......\n",
    "\n",
    "数据来源是 tushare.org 提供的免费数据, 实际采用的后复权的日线数据,  依赖的工具是python+numpy 以及 spark的聚类算法, 聚类计算是关键, 对比了tensorflow-gpu 和 spark,\n",
    "   发现单机模式下计算速度差别不大, 而前者还使用支持cuda的显卡, 这样对照来看, spark还是要牛逼些, 毕竟还天生支持集群的, tensorflow怎么在集群中使用还不知道, 肯定是支持, 但不会用.\n",
    "\n",
    " 下面是操作方法, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path=d:\\dev\\Anaconda3\\Scripts;d:\\dev\\Anaconda3;C:\\WINDOWS\\system32;C:\\WINDOWS\n",
    "# set QT_PLUGIN_PATH=d:\\dev\\Anaconda3\\Library\\plugins\n",
    "import sys\n",
    "from importlib import reload\n",
    "sys.path.insert(0, 'e:/worksrc/pycode/stock')\n",
    "import tushare_ut as tu\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 下载股票列表\n",
    "k_data_path = \"e:/stock/list11\"\n",
    "stocklist_file = 'e:/stock/stocklist.txt'\n",
    "\n",
    "tu.downtushare_stocklist(k_data_path) # 参数为要保存数据的文件名\n",
    "\n",
    "# 2. 全量下载k线数据， 第一个参数为输出数据的目录， 最后一个为并发进程数\n",
    "tu.downtushare_hday(k_data_path, stocklist_file , process_count=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 增量下载数据\n",
    "tu.ts_down_increasely(k_data_path, process_count=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 拆分k线为线段， 以及后续走势的标签数据\n",
    "lines_file=\"e:/stock/lines0330.txt\"\n",
    "tags_file = \"e:/stock/tag0330.txt\"\n",
    "k_data_path = \"e:/stock/list11\"\n",
    "spark_cluster_out_path = \"e:/stock/clustering_out\"\n",
    "spark_kmeans_model_path = \"e:/stock/kmeans_spark_model\"\n",
    "num_clusters = 120 #聚类数量\n",
    "tu.split_k_data(k_data_path, #k线数据文件所在目录\n",
    "                 lines_file, # 输出线段文件\n",
    "                 tags_file, # 输出标记文件， 行数与前一个文件相同， 格式 code date maxclose minclose\n",
    "                 lsize=30,  # 线段长度\n",
    "                 skip=5,    # 滚动条约的数量\n",
    "                 nextdays=6 # 标签走势取线段的长度\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 对线段数据进行聚类， 这部分是在spark-shell 中执行的 scala 代码\n",
    "# \n",
    "'''\n",
    "bin\\spark-shell --driver-memory=4g\n",
    "\n",
    "\n",
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val model_path=\"e:/stock/kmeans_spark_model\"\n",
    "val lines_path=\"e:/stock/lines0330.txt\"\n",
    "val output_path=\"e:/stock/clustering_out\"\n",
    "\n",
    "val numClusters = 120\n",
    "val numIterations = 100\n",
    "\n",
    "// Load and parse the data\n",
    "val data = sc.textFile(lines_path)\n",
    "val parsedData = data.map(s => Vectors.dense(s.split(\" \").map(_.toDouble)))\n",
    "parsedData.cache\n",
    "parsedData.first\n",
    "parsedData.count\n",
    "\n",
    "val clusters = KMeans.train(parsedData, numClusters, numIterations)\n",
    "clusters.save(sc, model_path)\n",
    "val sameModel = KMeansModel.load(sc, model_path)\n",
    "\n",
    "val out = sameModel.predict(parsedData)\n",
    "out.saveAsTextFile(output_path)\n",
    "\n",
    "// test different num_clusters and compare costs\n",
    "//val ks:Array[Int] = Array(10,15, 20, 25, 30, 50)\n",
    "//val ks:Array[Int] = Array(60,70,100,150)\n",
    "\n",
    "//ks.foreach(cluster => {\n",
    "// val model:KMeansModel = KMeans.train(parsedData, cluster, numIterations)\n",
    "// val ssd = model.computeCost(parsedData)\n",
    "// println(\"sum of squared distances of points to their nearest center when k=\" + cluster + \" -> \"+ ssd)\n",
    "//})\n",
    "\n",
    "// here is what I added to predict data points that are within the clusters\n",
    "//sameModel.predict(parsedData).foreach(println)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 对聚类结果进行统计\n",
    "lines, tag, clusters = tu.load_cluster_datasets(lines_file, # 线段数据文件\n",
    "                                                tags_file, # 标签数据文件\n",
    "                                                spark_cluster_out_path)\n",
    "win_point, lose_point = 5.0, 0.0\n",
    "centers = tu.check_clusters(tag, clusters, lines, num_clusters, win_point, lose_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check_clusters 输出的内容如下:\n",
    " ...\n",
    " cluster 58  count: 1706 win: 58.44 lose 18.93\n",
    " cluster 97  count: 3976 win: 58.63 lose 12.37\n",
    " cluster 25  count: 3472 win: 58.78 lose 12.99\n",
    " cluster 27  count: 1335 win: 59.10 lose 16.78\n",
    " cluster 114  count: 733 win: 59.62 lose 23.06\n",
    " cluster 64  count: 2443 win: 62.05 lose 13.67\n",
    " cluster 51  count: 1692 win: 63.06 lose 20.92\n",
    " cluster 113  count: 1660 win: 67.77 lose 13.49\n",
    " cluster 91  count: 2491 win: 73.30 lose 8.87\n",
    " cluster 35  count: 1333 win: 73.82 lose 11.33\n",
    " cluster 31  count: 1 win: 100.00 lose 0.00\n",
    "\n",
    "各列数字分别是： 分类编号， 线段数量， 胜率（maxclose>=5.0), 失败率（maxclose<=0.0), 输出结果按照win排序\n",
    "可以看到后边的4 5个类别有较高的胜率和低的失败率，  这就是理想中的形态了吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 继续分析结果， 将较高胜率的类别的均值线画出来看看\n",
    "tu.draw_centers(centers, [35, 91, 113, 51, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git exclude:\n",
    "*/target/*\n",
    "*/.ipynb_checkpoints/*\n",
    "*/__pycache__/*\n",
    "*.class\n",
    ".idea/\n",
    "*.iml\n",
    "*.pyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 下面是日常选股要做的事情， 首先是增量更新k线数据\n",
    "import sys\n",
    "from importlib import reload\n",
    "sys.path.insert(0, 'e:/worksrc/pycode/stock')\n",
    "import tushare_ut as tu\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "k_data_path = \"e:/stock/list11\"\n",
    "spark_cluster_out_path = \"e:/stock/clustering_out\"\n",
    "spark_kmeans_model_path = \"e:/stock/kmeans_spark_model\"\n",
    "day_lines_file = \"e:/stock/day_lines%s.txt\"%(time.strftime(\"%m%d\"))\n",
    "day_tags_file = \"e:/stock/day_tags%s.txt\"%(time.strftime(\"%m%d\"))\n",
    "\n",
    "# 7.1 增量下载k线数据\n",
    "tu.ts_down_increasely(k_data_path, process_count=8)\n",
    "\n",
    "# 7.2 拆分线段\n",
    "mindate = 20180101\n",
    "lsize = 30\n",
    "skip = 2\n",
    "tu.daily_split_k_data(k_data_path, day_lines_file, day_tags_file, mindate, lsize, skip)\n",
    "\n",
    "# 7.3 使用 spark 已有的model对线段进行分类\n",
    "'''\n",
    "bin\\spark-shell --driver-memory=4g\n",
    "\n",
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val model_path=\"e:/stock/kmeans_spark_model\"\n",
    "val lines_path=\"e:/stock/day_lines0330.txt\"\n",
    "val output_path=\"e:/stock/clustering_out_0330\"\n",
    "\n",
    "val data = sc.textFile(lines_path)\n",
    "val parsedData = data.map(s => Vectors.dense(s.split(\" \").map(_.toDouble)))\n",
    "parsedData.cache\n",
    "parsedData.first\n",
    "parsedData.count\n",
    "\n",
    "val sameModel = KMeansModel.load(sc, model_path)\n",
    "\n",
    "val out = sameModel.predict(parsedData)\n",
    "out.saveAsTextFile(output_path)\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
