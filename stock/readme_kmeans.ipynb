{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*这是一个股票数据分析方法的说明, 基本思路如下:*\n",
    "1. 历史数据中, 总有很多相似形状的走势, 这些形状有一些是可能预示后边走势的概率的\n",
    "2. 形状的处理上, 通过滚动切片的方式分割日k线数据, 并以每段的第一个k线收盘价作为基数, 整个线段只保留收盘价相对基数的涨幅数据,\n",
    "      这实际是一个归一化的处理过程, 股票和时段 价格的差异都抹去了, 只保留走势的形状片段\n",
    "3. 每个片段实际上就是一个向量, 且所有的片段长度相同, 起始点都是0, 所以以kmeans聚类方法对数据进行分类的话, 实际上就是将相近形状的片段聚类\n",
    "4. 每个片段都对应后续一段时间的走势情况, 保留n个交易日的最高价和最低价与片段最后一个收盘价的涨幅\n",
    "5. 使用聚类计算的结果, 对后续走势的情况进行分类, 然后统计每一类的最高 最低涨幅的相对某个值的分布, 能发现某些类别中的分布有明显不同\n",
    "6. 对分布比例相对突出的类别, 画出其均线形状, 可以发现有比较近似的形状\n",
    "7. 根据这类形状, 继续以之前聚类的模型对最新的数据进行分类, 寻找相似的形状, 这是一种选股方法\n",
    "\n",
    "  是否有效, 还在验证过程中......\n",
    "\n",
    "数据来源是 tushare.org 提供的免费数据, 实际采用的后复权的日线数据,  依赖的工具是python+numpy 以及 spark的聚类算法, 聚类计算是关键, 对比了tensorflow-gpu 和 spark,\n",
    "   发现单机模式下计算速度差别不大, 而前者还使用支持cuda的显卡, 这样对照来看, spark还是要牛逼些, 毕竟还天生支持集群的, tensorflow怎么在集群中使用还不知道, 肯定是支持, 但不会用.\n",
    "\n",
    " 下面是操作方法, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path=d:\\dev\\Anaconda3\\Scripts;d:\\dev\\Anaconda3;C:\\WINDOWS\\system32;C:\\WINDOWS\n",
    "# set QT_PLUGIN_PATH=d:\\dev\\Anaconda3\\Library\\plugins\n",
    "import sys\n",
    "from importlib import reload\n",
    "sys.path.insert(0, 'e:/worksrc/pycode/stock')\n",
    "import tushare_ut as tu\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 下载股票列表\n",
    "k_data_path = \"e:/stock/list11\"\n",
    "stocklist_file = 'e:/stock/stocklist.txt'\n",
    "\n",
    "tu.downtushare_stocklist(k_data_path) # 参数为要保存数据的文件名\n",
    "\n",
    "# 2. 全量下载k线数据， 第一个参数为输出数据的目录， 最后一个为并发进程数\n",
    "tu.downtushare_hday(k_data_path, stocklist_file , process_count=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 增量下载数据\n",
    "tu.ts_down_increasely(k_data_path, process_count=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 拆分k线为线段， 以及后续走势的标签数据\n",
    "lines_file=\"e:/stock/2010_lines0330.txt\"\n",
    "tags_file = \"e:/stock/2010_tag0330.txt\"\n",
    "k_data_path = \"e:/stock/list11\"\n",
    "spark_cluster_out_path = \"e:/stock/clustering_out\"\n",
    "spark_kmeans_model_path = \"e:/stock/kmeans_spark_model\"\n",
    "if os.path.exists(spark_cluster_out_path):\n",
    "    shutil.rmtree(spark_cluster_out_path)\n",
    "if os.path.exists(spark_kmeans_model_path):\n",
    "    shutil.rmtree(spark_kmeans_model_path)\n",
    "num_clusters = 120 #聚类数量\n",
    "tu.split_k_data(k_data_path, #k线数据文件所在目录\n",
    "                 lines_file, # 输出线段文件\n",
    "                 tags_file, # 输出标记文件， 行数与前一个文件相同， 格式 code date maxclose minclose\n",
    "                 lsize=30,  # 线段长度\n",
    "                 skip=5,    # 滚动条约的数量\n",
    "                 nextdays=6, # 标签走势取线段的长度\n",
    "                 start_year=\"2010\" #数据起始年份\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 对线段数据进行聚类， 这部分是在spark-shell 中执行的 scala 代码\n",
    "# \n",
    "'''\n",
    "bin\\spark-shell --driver-memory=4g\n",
    "\n",
    "\n",
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val model_path=\"e:/stock/kmeans_spark_model\"\n",
    "val lines_path=\"e:/stock/2010_lines0330.txt\"\n",
    "val output_path=\"e:/stock/clustering_out\"\n",
    "\n",
    "val numClusters = 120\n",
    "val numIterations = 100\n",
    "\n",
    "// Load and parse the data\n",
    "val data = sc.textFile(lines_path)\n",
    "val parsedData = data.map(s => Vectors.dense(s.split(\" \").map(_.toDouble)))\n",
    "parsedData.cache\n",
    "parsedData.first\n",
    "parsedData.count\n",
    "\n",
    "val clusters = KMeans.train(parsedData, numClusters, numIterations)\n",
    "clusters.save(sc, model_path)\n",
    "val sameModel = KMeansModel.load(sc, model_path)\n",
    "\n",
    "val out = sameModel.predict(parsedData)\n",
    "out.saveAsTextFile(output_path)\n",
    "\n",
    "// test different num_clusters and compare costs\n",
    "//val ks:Array[Int] = Array(10,15, 20, 25, 30, 50)\n",
    "//val ks:Array[Int] = Array(60,70,100,150)\n",
    "\n",
    "//ks.foreach(cluster => {\n",
    "// val model:KMeansModel = KMeans.train(parsedData, cluster, numIterations)\n",
    "// val ssd = model.computeCost(parsedData)\n",
    "// println(\"sum of squared distances of points to their nearest center when k=\" + cluster + \" -> \"+ ssd)\n",
    "//})\n",
    "\n",
    "// here is what I added to predict data points that are within the clusters\n",
    "//sameModel.predict(parsedData).foreach(println)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cluster 13  count: 4 win: 0.00 lose 100.00\n",
      " cluster 43  count: 22 win: 13.64 lose 54.55\n",
      " cluster 27  count: 30084 win: 16.48 lose 23.97\n",
      " cluster 45  count: 32981 win: 17.54 lose 22.62\n",
      " cluster 22  count: 26519 win: 17.67 lose 24.23\n",
      " cluster 33  count: 31274 win: 18.15 lose 23.07\n",
      " cluster 39  count: 26703 win: 20.60 lose 21.98\n",
      " cluster 101  count: 19781 win: 21.70 lose 23.80\n",
      " cluster 86  count: 20969 win: 22.35 lose 21.99\n",
      " cluster 73  count: 14998 win: 23.20 lose 20.98\n",
      " cluster 5  count: 26482 win: 23.48 lose 21.97\n",
      " cluster 107  count: 26783 win: 23.58 lose 21.37\n",
      " cluster 84  count: 25689 win: 23.64 lose 20.39\n",
      " cluster 7  count: 17527 win: 23.75 lose 20.13\n",
      " cluster 63  count: 25936 win: 23.94 lose 22.16\n",
      " cluster 76  count: 15265 win: 24.38 lose 22.26\n",
      " cluster 113  count: 27881 win: 24.46 lose 21.50\n",
      " cluster 89  count: 4 win: 25.00 lose 50.00\n",
      " cluster 8  count: 22916 win: 25.21 lose 20.23\n",
      " cluster 64  count: 17789 win: 25.30 lose 20.55\n",
      " cluster 2  count: 22820 win: 25.43 lose 20.61\n",
      " cluster 49  count: 15015 win: 26.08 lose 20.00\n",
      " cluster 38  count: 17860 win: 26.79 lose 21.25\n",
      " cluster 66  count: 59 win: 27.12 lose 44.07\n",
      " cluster 116  count: 16791 win: 27.35 lose 22.53\n",
      " cluster 62  count: 14127 win: 27.44 lose 19.87\n",
      " cluster 11  count: 10753 win: 28.36 lose 19.46\n",
      " cluster 100  count: 9828 win: 28.74 lose 21.27\n",
      " cluster 99  count: 9648 win: 29.26 lose 22.36\n",
      " cluster 25  count: 16022 win: 29.90 lose 21.18\n",
      " cluster 16  count: 10950 win: 29.96 lose 19.33\n",
      " cluster 40  count: 12711 win: 30.31 lose 19.74\n",
      " cluster 98  count: 11681 win: 30.76 lose 17.41\n",
      " cluster 87  count: 17049 win: 30.80 lose 21.18\n",
      " cluster 14  count: 13402 win: 30.91 lose 19.67\n",
      " cluster 93  count: 12600 win: 30.98 lose 19.25\n",
      " cluster 70  count: 15792 win: 31.15 lose 20.42\n",
      " cluster 24  count: 15530 win: 31.33 lose 23.21\n",
      " cluster 67  count: 6928 win: 31.63 lose 21.03\n",
      " cluster 119  count: 10238 win: 31.63 lose 21.47\n",
      " cluster 85  count: 14261 win: 32.39 lose 22.45\n",
      " cluster 47  count: 7703 win: 32.66 lose 22.04\n",
      " cluster 110  count: 10751 win: 32.82 lose 19.24\n",
      " cluster 71  count: 10691 win: 33.26 lose 21.63\n",
      " cluster 23  count: 3 win: 33.33 lose 33.33\n",
      " cluster 20  count: 12683 win: 33.60 lose 20.97\n",
      " cluster 52  count: 15134 win: 33.74 lose 17.96\n",
      " cluster 56  count: 11683 win: 34.61 lose 21.48\n",
      " cluster 35  count: 8504 win: 34.68 lose 18.74\n",
      " cluster 30  count: 7197 win: 35.78 lose 19.37\n",
      " cluster 12  count: 6011 win: 36.10 lose 23.59\n",
      " cluster 58  count: 4135 win: 36.11 lose 21.55\n",
      " cluster 37  count: 7909 win: 37.50 lose 20.63\n",
      " cluster 59  count: 824 win: 37.62 lose 35.44\n",
      " cluster 79  count: 10161 win: 38.55 lose 21.94\n",
      " cluster 18  count: 4153 win: 38.77 lose 22.18\n",
      " cluster 72  count: 6886 win: 38.92 lose 20.91\n",
      " cluster 91  count: 114 win: 39.47 lose 27.19\n",
      " cluster 48  count: 7304 win: 39.69 lose 17.85\n",
      " cluster 78  count: 10 win: 40.00 lose 50.00\n",
      " cluster 65  count: 1280 win: 40.08 lose 30.16\n",
      " cluster 109  count: 3454 win: 40.13 lose 25.16\n",
      " cluster 0  count: 4323 win: 40.30 lose 20.89\n",
      " cluster 26  count: 251 win: 40.64 lose 29.08\n",
      " cluster 68  count: 1788 win: 40.66 lose 26.12\n",
      " cluster 80  count: 49 win: 40.82 lose 20.41\n",
      " cluster 17  count: 712 win: 41.15 lose 27.67\n",
      " cluster 1  count: 34 win: 41.18 lose 38.24\n",
      " cluster 36  count: 6000 win: 41.25 lose 20.70\n",
      " cluster 115  count: 58 win: 41.38 lose 24.14\n",
      " cluster 88  count: 5867 win: 41.40 lose 20.62\n",
      " cluster 103  count: 281 win: 41.64 lose 27.05\n",
      " cluster 53  count: 7671 win: 41.64 lose 18.88\n",
      " cluster 92  count: 4125 win: 41.70 lose 19.81\n",
      " cluster 3  count: 182 win: 41.76 lose 21.98\n",
      " cluster 21  count: 415 win: 41.93 lose 26.51\n",
      " cluster 75  count: 3040 win: 42.53 lose 24.01\n",
      " cluster 90  count: 1551 win: 42.68 lose 37.01\n",
      " cluster 15  count: 7859 win: 42.75 lose 16.30\n",
      " cluster 6  count: 86 win: 43.02 lose 36.05\n",
      " cluster 112  count: 1190 win: 43.03 lose 29.41\n",
      " cluster 111  count: 4158 win: 43.07 lose 23.91\n",
      " cluster 32  count: 3212 win: 43.28 lose 24.19\n",
      " cluster 44  count: 272 win: 43.38 lose 31.62\n",
      " cluster 74  count: 152 win: 43.42 lose 33.55\n",
      " cluster 31  count: 1951 win: 44.23 lose 20.45\n",
      " cluster 10  count: 3038 win: 44.37 lose 25.74\n",
      " cluster 57  count: 310 win: 44.52 lose 27.10\n",
      " cluster 97  count: 629 win: 44.99 lose 29.57\n",
      " cluster 81  count: 5570 win: 45.03 lose 22.32\n",
      " cluster 34  count: 4353 win: 45.69 lose 23.32\n",
      " cluster 69  count: 3368 win: 46.11 lose 19.77\n",
      " cluster 29  count: 3149 win: 46.24 lose 21.98\n",
      " cluster 94  count: 30 win: 46.67 lose 30.00\n",
      " cluster 9  count: 1886 win: 46.77 lose 24.23\n",
      " cluster 42  count: 1369 win: 46.82 lose 26.66\n",
      " cluster 104  count: 3195 win: 46.98 lose 24.04\n",
      " cluster 95  count: 996 win: 47.19 lose 27.71\n",
      " cluster 118  count: 3013 win: 47.56 lose 24.63\n",
      " cluster 54  count: 323 win: 48.61 lose 23.84\n",
      " cluster 61  count: 595 win: 48.91 lose 32.77\n",
      " cluster 46  count: 93 win: 49.46 lose 29.03\n",
      " cluster 60  count: 1207 win: 51.95 lose 26.01\n",
      " cluster 82  count: 1689 win: 52.40 lose 20.84\n",
      " cluster 4  count: 1169 win: 52.61 lose 25.15\n",
      " cluster 28  count: 333 win: 54.35 lose 26.73\n",
      " cluster 105  count: 2039 win: 54.78 lose 21.24\n",
      " cluster 55  count: 4060 win: 55.79 lose 12.00\n",
      " cluster 117  count: 3854 win: 56.33 lose 12.48\n",
      " cluster 102  count: 1365 win: 57.44 lose 24.69\n",
      " cluster 19  count: 3393 win: 57.68 lose 13.97\n",
      " cluster 106  count: 2254 win: 58.83 lose 13.58\n",
      " cluster 96  count: 2501 win: 61.66 lose 13.39\n",
      " cluster 41  count: 1811 win: 63.17 lose 14.58\n",
      " cluster 77  count: 1334 win: 66.04 lose 18.52\n",
      " cluster 108  count: 1507 win: 67.42 lose 13.34\n",
      " cluster 51  count: 2804 win: 72.72 lose 9.09\n",
      " cluster 50  count: 1082 win: 78.10 lose 9.89\n",
      " cluster 83  count: 2 win: 100.00 lose 0.00\n",
      " cluster 114  count: 1 win: 100.00 lose 0.00\n"
     ]
    }
   ],
   "source": [
    "# 6. 对聚类结果进行统计\n",
    "lines_file=\"e:/stock/2010_lines0330.txt\"\n",
    "tags_file = \"e:/stock/2010_tag0330.txt\"\n",
    "spark_cluster_out_path = \"e:/stock/clustering_out\"\n",
    "num_clusters = 120 #聚类数量\n",
    "lines, tag, clusters = tu.load_cluster_datasets(lines_file, # 线段数据文件\n",
    "                                                tags_file, # 标签数据文件\n",
    "                                                spark_cluster_out_path)\n",
    "win_point, lose_point = 5.0, 0.0\n",
    "centers = tu.check_clusters(tag, clusters, lines, num_clusters, win_point, lose_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check_clusters 输出的内容如下:\n",
    " ...\n",
    " cluster 58  count: 1706 win: 58.44 lose 18.93\n",
    " cluster 97  count: 3976 win: 58.63 lose 12.37\n",
    " cluster 25  count: 3472 win: 58.78 lose 12.99\n",
    " cluster 27  count: 1335 win: 59.10 lose 16.78\n",
    " cluster 114  count: 733 win: 59.62 lose 23.06\n",
    " cluster 64  count: 2443 win: 62.05 lose 13.67\n",
    " cluster 51  count: 1692 win: 63.06 lose 20.92\n",
    " cluster 113  count: 1660 win: 67.77 lose 13.49\n",
    " cluster 91  count: 2491 win: 73.30 lose 8.87\n",
    " cluster 35  count: 1333 win: 73.82 lose 11.33\n",
    " cluster 31  count: 1 win: 100.00 lose 0.00\n",
    "\n",
    "各列数字分别是： 分类编号， 线段数量， 胜率（maxclose>=5.0), 失败率（maxclose<=0.0), 输出结果按照win排序\n",
    "可以看到后边的4 5个类别有较高的胜率和低的失败率，  这就是理想中的形态了吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6.1 继续分析结果， 将较高胜率的类别的均值线画出来看看\n",
    "tu.draw_centers(centers, [50, 51, 108, 77, 41, 96])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git exclude:\n",
    "*/target/*\n",
    "*/.ipynb_checkpoints/*\n",
    "*/__pycache__/*\n",
    "*.class\n",
    ".idea/\n",
    "*.iml\n",
    "*.pyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 下面是日常选股要做的事情， 首先是增量更新k线数据\n",
    "import sys\n",
    "from importlib import reload\n",
    "sys.path.insert(0, 'e:/worksrc/pycode/stock')\n",
    "import tushare_ut as tu\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "k_data_path = \"e:/stock/list11\"\n",
    "cluster_result_path = \"e:/stock/clustering_out_0330\"\n",
    "spark_kmeans_model_path = \"e:/stock/kmeans_spark_model\"\n",
    "day_lines_file = \"e:/stock/day_lines%s.txt\"%(time.strftime(\"%m%d\"))\n",
    "day_tags_file = \"e:/stock/day_tags%s.txt\"%(time.strftime(\"%m%d\"))\n",
    "\n",
    "# 7.1 增量下载k线数据\n",
    "#tu.ts_down_increasely(k_data_path, process_count=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# 7.2 拆分线段\n",
    "start_year = \"2018\"\n",
    "lsize = 30\n",
    "skip = 2\n",
    "reload(tu)\n",
    "tu.daily_split_k_data(k_data_path, start_year, day_lines_file, day_tags_file, lsize, skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3 使用 spark 已有的model对线段进行分类\n",
    "\n",
    "bin\\spark-shell --driver-memory=4g\n",
    "\n",
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val model_path=\"e:/stock/kmeans_spark_model\"\n",
    "val lines_path=\"e:/stock/day_lines0330.txt\"\n",
    "val cluster_result_path=\"e:/stock/clustering_out_0330\"\n",
    "\n",
    "val data = sc.textFile(lines_path)\n",
    "val parsedData = data.map(s => Vectors.dense(s.split(\" \").map(_.toDouble)))\n",
    "parsedData.cache\n",
    "parsedData.first\n",
    "parsedData.count\n",
    "\n",
    "val sameModel = KMeansModel.load(sc, model_path)\n",
    "\n",
    "val out = sameModel.predict(parsedData)\n",
    "out.saveAsTextFile(cluster_result_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 寻找符合高概率的票, 得从上面的统计结果中获取类别编号, 这里的结果是 [50, 51, 108, 77, 41, 96]\n",
    "clusters_want = [50, 51, 108, 77, 41, 96]\n",
    "tu.daily_check_clusters(tags_file, cluster_result_path, clusters_want)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
